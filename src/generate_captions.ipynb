{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.vct0 import VCT0Prefix\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " Using MLP \n",
      "\n",
      "\n",
      "MLP(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=768, out_features=10240, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=10240, out_features=20480, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    \"prefix_length\": 10,\n",
    "    \"prefix_size\": 768,\n",
    "    \"mapping_type\": \"mlp\",\n",
    "    \"model_version\": \"bigscience/T0_3B\"\n",
    "}\n",
    "model = VCT0Prefix(**args)\n",
    "checkpoint = torch.load(\"/home/rsa39/rds/rds-mlmi-2020-21-xyBFuSj0hm0/MLMI.2021-22/rsa39/project/RAVQA/Experiments/VC-T0_3B-Conceptual-Captions-MLP-Prefix10/train/saved_model/model_04.ckpt\", map_location=torch.device('cpu'))\n",
    "# checkpoint = torch.load(\"/home/rsa39/rds/rds-mlmi-2020-21-xyBFuSj0hm0/MLMI.2021-22/rsa39/project/RAVQA/Experiments/VC-T0pp-Conceptual-Captions-MLP-Prefix5/train/saved_model/model_01.ckpt\", map_location=torch.device('cpu'))\n",
    "state_dict = {key[6:]: value for key, value in checkpoint['state_dict'].items()}\n",
    "model.load_state_dict(state_dict)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0_3B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-f878f8eac05c77e9\n",
      "Reusing dataset parquet (/home/rsa39/.cache/huggingface/datasets/parquet/default-f878f8eac05c77e9/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d342e027f84c20aa50c315bb7c59a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "con_caps = load_dataset(\n",
    "    \"parquet\",\n",
    "    data_files={\n",
    "        \"val\": \"/home/rsa39/rds/rds-mlmi-2020-21-xyBFuSj0hm0/MLMI.2021-22/rsa39/project/RAVQA/data/conceptual_captions/pre-extracted-features/conceptual_captions_ViT-L_14@336px_validation.parquet\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_of_interest = [\n",
    "    \"https://i.pinimg.com/736x/b3/1d/79/b31d7983262164d4025e2f95eb52172d--shakespeare-characters-shakespeare-love.jpg\",\n",
    "    \"https://ak0.picdn.net/shutterstock/videos/5526380/thumb/1.jpg\",\n",
    "    \"https://thumb1.shutterstock.com/image-photo/stock-photo-pair-of-new-bright-orange-modern-sneakers-isolated-on-a-white-background-450w-598949420.jpg\",\n",
    "    \"http://0345ed7.netsolhost.com/images/no_drones.jpg\",\n",
    "    \"http://c8.alamy.com/comp/KFK4TJ/zoo-keeper-feeding-apples-to-an-elephant-in-captivity-spain-KFK4TJ.jpg\"\n",
    "]\n",
    "\n",
    "text_list = [\n",
    "    \"Summarize: <extra_id_0>\",\n",
    "    # \"Reorder the words in this sentence: <extra_id_0>\",\n",
    "    # \"Write a dialogue that matches this summary: <extra_id_0>\",\n",
    "    # \"Write an expanded news article with plausible details from the following summary: <extra_id_0>\",\n",
    "    # \"We have the sentence: <extra_id_0>; Extract all the key concepts:\",\n",
    "    # \"Humans can easily string together abstract concepts to form a coherent sentence. For example, with the concepts <extra_id_0>, a simple sentence can be\",\n",
    "    # \"Ignoring the order of the concepts: <extra_id_0>; Generate a sentence with all the concepts:\",\n",
    "    # \"Extract the answer to the question from the following context. Question: Is the cat wearing a crown? Context: <extra_id_0>\",\n",
    "    # \"Read the following context and choose the best option to answer the question.\\nContext: <extra_id_0>\\nQuestion: what does the sign say? \\nOptions:\\nA. Unattended vehicles will be towed at the owner's expense\\nB. This is ross street\\nC. No camping\\nD. a cat lives here\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_text_input = \"<pad> A picture of\"\n",
    "decoder_input = tokenizer.encode(decoder_text_input, return_tensors=\"pt\")[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "We have the sentence: <extra_id_0>; Extract all the key concepts:\n",
      "\n",
      "\n",
      "person feeding apples to an elephant in captivity.\n",
      "a young elephant with a bottle of water.</s>\n",
      "a skull and cross bones pirate flag blows in the wind.\n",
      "a pirate ship with a flag flying.</s>\n",
      "combines famous scenes from plays with the internet 's favorite animal.\n",
      "a cat with a crown.</s>\n",
      "picture of presidents with sign in the foreground.\n",
      "a man with a gun pointing at a sign.</s>\n"
     ]
    }
   ],
   "source": [
    "for text in text_list:\n",
    "    print(\"\\n\\n\" + text + \"\\n\\n\")\n",
    "    text_inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    for entry in con_caps[\"val\"]:\n",
    "        if entry['image_url'][0] in images_of_interest:\n",
    "            print(entry[\"caption\"][0])\n",
    "            clip_embedding = entry[\"clip_embeddings\"]\n",
    "            torch_clip_embedding = torch.tensor(clip_embedding).view(1, 1, -1)\n",
    "            print(tokenizer.decode(model.generate(prefix=torch_clip_embedding, max_length=50)[0]))\n",
    "            print(tokenizer.decode(model.generate(prefix=torch_clip_embedding, question_tokens=text_inputs.input_ids, question_mask=text_inputs.attention_mask, max_length=50)[0]))\n",
    "            print(tokenizer.decode(model.generate(prefix=torch_clip_embedding, question_tokens=text_inputs.input_ids, question_mask=text_inputs.attention_mask, decoder_input_ids=decoder_input, max_length=50)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.2 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8fa29f82c12e02e3dc7486d4b89654c74c1a58af68722c485e48721eca2c3841"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
